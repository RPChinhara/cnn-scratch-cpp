> Standard Transformer (Original)
- Stacked standard Transformer (Original)?
- Best rl model (dqn?)
- BERT, RoBERTa, ALBERT (Encoder-Only Transformers)
- GPT (GPT-2, GPT-3, GPT-4, ChatGPT), LLaMA (Decoder-Only Transformers)
- T5, BART, mT5 (Encoder-Decoder Transformers)
- LLMs (Focusing on English, and science, what are standards?)
- Vision-Language Models
- DeepSeek
- MoE
- Story Visualization
- Autoencoder
- GAN
- YOLO
- Mamba
- Diffusion Transformers
    - DiT, UViT
    - Diffusion models
    - DALL·E
- Vision transformer (MNIST for the dataset?)
    - ViT, Swin Transformer, DeiT
- AlphaFold
- Hybrid Transformers (Vision + NLP)
    - Multimodal learning (text + image)
    - Visual question answering
    - CLIP, DINO, Flamingo
- Reasoning
- AGI (Physics e.g., quantum gravity, time and entropy)

- How did they come up with attention or multiHeadAttention such as multiplying q and k, and later with v.
- What is Efficient Transformers?
- How to solve needs billions of parameters (weights, biasese)?
    - Use bits as weights and biases?
    - Use one integet to represent 10 weights since maximum value for a variable of type int is 2147483647 which has 10 digits each act as weights.
    - Use smaller floating values
    - Use like NumPy memmap – Loads large arrays from disk on demand. Called "Out-of-Core Computing" and "Memory Management Techniques".
- Make a pre-trained level models for image classification like ResNet and NLP like Llama

- Add new synapses (neurogenesis)
- Rewires neurons (synapses) dynamically (neuroplasticity)
- Stochastic firing: neurons fire randomly, even without direct input.
    - This adds "noise" to brain activity, helping with creativity, problem-solving, and flexibility.
    - It prevents the brain from getting stuck in fixed patterns (unlike AI, which follows strict mathematical rules).
    - AI models only activate when given input (no random self-firing).
    - AI doesn’t have spontaneous thought generation—it only predicts based on patterns.
- Instead of full network updates using gradient-based weight updates, each neuron should updates its own weights.
- Try dynamic neural architectures (where connections form and break like neuroplasticity).
- Consider graph-based AI models instead of just dense layers as it more similar to how neurons in brain are connected?
- Experiment with Hebbian learning rules instead of backpropagation.
- Build a dynamic neural network that adds/removes connections like the brain.
- Study Neuro-Symbolic AI to combine logic + deep learning.
- Explore spiking neural networks (SNNs) for biologically accurate learning.
- Modular Networks → Different specialized subnetworks for vision, language, reasoning, etc.
- Implement adaptive forgetting (removing useless data to free memory).
- Implement memory consolidation: Store long-term knowledge gradually.
- Add attention-based recall: Retrieve only relevant memories.
- Use adaptive forgetting: Remove unimportant details over time.
    - This allows AI to store knowledge persistently, like humans do.
    - Right now, AI only has short-term recall (via attention mechanisms). Humans, however, have:
    - Working memory (holds temporary thoughts).
    - Long-term memory (consolidates knowledge).
    - Episodic memory (stores past experiences).
- Dynamic Synapses: Instead of fixed 32x32 weight matrices, let synapses grow or shrink based on activity (like neuroplasticity).

- Maybe use cloud computig to train the model in the future, possibly resources will be limited later. Is that mean my code needs to be support Linux/Unix? Also, Use OpenGL?
- In the future, I need to save parameters like weights and biases, and keep updating these to make a better model like how transfer learning works.
    - I need to do this as soon as one can. Train a best model using same parameters, and keep updating these. I think once I created transformer I need to do this.
    - What happens if the model is overfitted? Add more training data so that losses will probably increase since model hasn't seen the datasets.
    - Search on what kind of datasets is chatGPT is trained on? Use these datasets to train mine.
    - However, sizes of weights and biases won't be same for all the models. What should I do?
    - I need to train for cv, nlp, rl?
        - I have to train using ImageNet for computer vision?
    - When I train it, do it on copy of dora, this way I can work on my project simultaneously.
    - Is transfer learning used repeadely to improve the model or just use pretrained model to train using unseen dataset to make a new model?
- I need to train embedding in order to create my own Word2Vec?
- I have to benchmark my model for different tasks, e.g., English (MMLU (EM), DROP (3-shot F1)), Code (HumanEval-Mul (Pass@1)), and Math (AIME 2024 (Pass@1)).
- If we can't parallelize backprop since each gradients depends on prevous ones because of the chain rule from math, we shouldn't be using it. Make alternative one.