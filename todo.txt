> Standard Transformer (Original)
> Draw a rectangle
- Stacked standard Transformer (Original)?
- Best rl model (dqn?)
- BERT, RoBERTa, ALBERT (Encoder-Only Transformers)
- GPT (GPT-2, GPT-3, GPT-4, ChatGPT), LLaMA (Decoder-Only Transformers)
- T5, BART, mT5 (Encoder-Decoder Transformers)
- LLMs (Focusing on English, and science, what are standards?)
- Vision-Language Models
- DeepSeek
- MoE
- Story Visualization
- Autoencoder
- GAN
- YOLO
- Mamba
- Diffusion Transformers
    - DiT, UViT
    - Diffusion models
    - DALL·E
- Vision transformer (MNIST for the dataset?)
    - ViT, Swin Transformer, DeiT
- AlphaFold
- Hybrid Transformers (Vision + NLP)
    - Multimodal learning (text + image)
    - Visual question answering
    - CLIP, DINO, Flamingo
- Reasoning
- AGI (Physics e.g., quantum gravity, time and entropy)

- How to solve needs billions of parameters (weights, biasese)?
    - Use bits as weights and biases?
    - Use one integet to represent 10 weights since maximum value for a variable of type int is 2147483647 which has 10 digits each act as weights.
    - Use smaller floating values
    - Use like NumPy memmap – Loads large arrays from disk on demand. Called "Out-of-Core Computing" and "Memory Management Techniques".

- Add new synapses (neurogenesis)
- Dynamic Synapses: Instead of fixed 32x32 weight matrices, let synapses grow or shrink based on activity where connections form and break like neuroplasticity (like neuroplasticity).
- Stochastic firing: neurons fire randomly, even without direct input.
    - This adds "noise" to brain activity, helping with creativity, problem-solving, and flexibility.
    - It prevents the brain from getting stuck in fixed patterns (unlike AI, which follows strict mathematical rules).
    - AI models only activate when given input (no random self-firing).
    - AI doesn’t have spontaneous thought generation—it only predicts based on patterns.
- Instead of full network updates using gradient-based weight updates, each neuron should updates its own weights.
- Consider graph-based AI models instead of just dense layers as it more similar to how neurons in brain are connected?
- Experiment with Hebbian learning rules instead of backpropagation.
- Build a dynamic neural network that adds/removes connections like the brain.
- Study Neuro-Symbolic AI to combine logic + deep learning.
- Explore spiking neural networks (SNNs) for biologically accurate learning.
- Modular Networks → Different specialized subnetworks for vision, language, reasoning, etc.
- Implement adaptive forgetting (removing useless data to free memory).
- Implement memory consolidation: Store long-term knowledge gradually.
- Add attention-based recall: Retrieve only relevant memories.
- Use adaptive forgetting: Remove unimportant details over time.
    - This allows AI to store knowledge persistently, like humans do.
    - Right now, AI only has short-term recall (via attention mechanisms). Humans, however, have:
    - Working memory (holds temporary thoughts).
    - Long-term memory (consolidates knowledge).
    - Episodic memory (stores past experiences).

- Maybe use cloud computig to train the model in the future, possibly resources will be limited later. Is that mean my code needs to be support Linux/Unix? Also, Use OpenGL?