-------------------- create_sequences() --------------------
Given following dataset, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], and I have to create sequence length of 3 it'd be
[1, 2, 3], [2, 3, 4], [3, 4, 5], [4, 5, 6], [5, 6, 7], [6, 7, 8], [7, 8, 9], [8, 9, 10] which is exactly what create_sequences() does.
However, inside forward() I decompose these so that I can process in batches like below,
[1, 2, 3, 4, 5, 6, 7, 8], [2, 3, 4, 5, 6, 7, 8, 9], and [3, 4, 5, 6, 7, 8, 9, 10].
So after all, I don't know if create_sequences() is really neccessaly I just did it since all the Python rnn examples do.

-------------------- BPTT --------------------

[This is when the model is many-to-one]

BPTT process for an RNN with two time steps. This shows how to calculate gradients respect to Whh.

h1 = fh(matmul(Wxh, x1) + matmul(Whh, h0) + bh)
h2 = fh(matmul(Wxh, x2) + matmul(Whh, h1) + bh)
y  = fy(matmul(Why, h2) + by)

loss function at time step T = 2: L = L(y, y_true)

T = 2
dL/dh2 = dL/dy ⋅ dy/dh2 where dy/dh2 = Why
Thus, dL/dh2 = dL/dy ⋅ Why
dL/dWhh (t = 2) = dL/dh2 ⋅ dh2/dWhh where dh2/dWhh = 1 - squared(h2) ⋅ transpose(h1)
Thus, dL/dWhh = dL/dh2 ⋅ 1 - squared(h2) ⋅ transpose(h1)

T = 1
dL/dh1 = dL/dh2 ⋅ dh2/dh1 where dh2/dh1 = 1 - squared(h2) ⋅ Whh
Thus, dL/dh1 = dL/dh2 ⋅ 1 - squared(h2) ⋅ Whh
dL/dWhh (t = 1) = dL/dh1 ⋅ h1/dWhh where dh1/dWhh = 1 - squared(h1) ⋅ transpose(h0)
Thus, dL/dWhh = dL/dh1 ⋅ 1 - squared(h1) ⋅ transpose(h0)

Total gradient with respect to Whh

dL/dWhh (t = 2) + dL/dWhh (t = 1)
dL/dWhh = (dL/dh1 ⋅ 1 - squared(h1) ⋅ transpose(h0)) + (dL/dh2 ⋅ 1 - squared(h2) ⋅ transpose(h1))


This is general equation of gradient with respect to Whh::
dL/dWhh = sum t from 1 to 2 dL/dht ⋅ dht/dWhh where dht/dWhh = 1 - squared(ht) ⋅ transpose(ht-1)
Thus, dL/dWhh = sum t from 1 to 2 dL/dht ⋅ 1 - squared(ht) ⋅ transpose(ht-1)

[When the model is either one-to-many or many-to-many things are bit different]

For simplicity, let's say I'm calculating dL/dh1. The equation is dL/dh1 + dL/dh2 ⋅ dh2/dh1 since h1 is contributing both
loss at time step = 1, and future time step in this case which is h2.

-------------------- BPTT2 --------------------

[Forward propagation]

z1 = matmul(Wxh, x1) + matmul(Whh, h0) + bh
h1 = act(z1)

z2 = matmul(Wxh, x2) + matmul(Whh, h1) + bh
h2 = act(z2)

z3 = matmul(Wxh, x3) + matmul(Whh, h2) + bh
h3 = act(z3)

z4 = matmul(Wxh, x4) + matmul(Whh, h3) + bh
h4 = act(z4)

y  = matmul(Why, h4) + by

Loss function at time step T = 4 is L = L(y, y_true)

f(x)  = x^2
f(x)' = 2x

f(2)  = 2^2  = 4
f(2)' = 2(2) = 4

[BPTT]

T = 4
dL/dh4 = dL/dy ⋅ dy/dh4 where dy/dh4 = Why
Thus, dL/dh4 = dL/dy ⋅ Why
dL/dWhh (t = 4) = dL/dh4 ⋅ dh4/dWhh where dh4/dWhh = act(z4)' ⋅ transpose(h3)
Thus, dL/dWhh (t = 4) = dL/dh4 ⋅ act(z4)' ⋅ transpose(h3)

T = 3
dL/dh3 = dL/dh4 ⋅ dh4/dh3 where dh4/dh3 = act(z4)' ⋅ Whh
Thus, dL/dh3 = dL/dh4 ⋅ 1 - squared(h4) ⋅ Whh
dL/dWhh (t = 3) = dL/dh3 ⋅ dh3/dWhh where dh3/dWhh = act(z3)' ⋅ transpose(h2)
Thus, dL/dWhh (t = 3) = dL/dh3 ⋅ act(z3)' ⋅ transpose(h2)

T = 2
dL/dh2 = dL/dh3 ⋅ dh3/dh2 where dh3/dh2 = act(z3)' ⋅ Whh
Thus, dL/dh2 = dL/dh3 ⋅ 1 - squared(h3) ⋅ Whh
dL/dWhh (t = 2) = dL/dh2 ⋅ dh2/dWhh where dh2/dWhh = act(z2)' ⋅ transpose(h1)
Thus, dL/dWhh (t = 2) = dL/dh2 ⋅ act(z2)' ⋅ transpose(h1)

T = 1
dL/dh1 = dL/dh2 ⋅ dh2/dh1 where dh2/dh1 = act(z2)' ⋅ Whh
Thus, dL/dh1 = dL/dh2 ⋅ 1 - squared(h2) ⋅ Whh
dL/dWhh (t = 1) = dL/dh1 ⋅ dh1/dWhh where dh1/dWhh = act(z1)' ⋅ transpose(h0)
Thus, dL/dWhh (t = 1) = dL/dh1 ⋅ act(z1)' ⋅ transpose(h0)

Total gradient with respect to Whh is,

dL/dWhh (t = 1) + dL/dWhh (t = 2) + dL/dWhh (t = 3) + dL/dWhh (t = 4)
dL/dWhh = (dL/dh1 ⋅ act(z1)' ⋅ transpose(h0)) + (dL/dh2 ⋅ act(z2)' ⋅ transpose(h1)) + (dL/dh3 ⋅ act(z3)' ⋅ transpose(h2)) + (dL/dh4 ⋅ act(z4)' ⋅ transpose(h3))

-------------------- LSTM BPTT --------------------

tensor concat1 = vstack({h_0, transpose(x_1)});

tensor z_f_1 = matmul(w_f, concat1) + b_f;
tensor f_1 = sigmoid(z_f_1);

tensor z_i_1 = matmul(w_i, concat1) + b_i;
tensor i_1 = sigmoid(z_i_1);

tensor z_c_tilde_1 = matmul(w_c, concat1) + b_c;
tensor c_tilde_1 = hyperbolic_tangent(z_c_tilde_1);

c_1 = f_1 * c_0 + i_1 * c_tilde_1;

tensor z_o_1 = matmul(w_o, concat1) + b_o;
tensor o_1 = sigmoid(z_o_1);

h_1 = o_1 * hyperbolic_tangent(c_1);
tensor y_1 = matmul(w_y, h_1) + b_y;

tensor concat2 = vstack({h_1, transpose(x_2)});

tensor z_f_2 = matmul(w_f, concat2) + b_f;
tensor f_2 = sigmoid(z_f_2);

tensor z_i_2 = matmul(w_i, concat2) + b_i;
tensor i_2 = sigmoid(z_i_2);

tensor z_c_tilde_2 = matmul(w_c, concat2) + b_c;
tensor c_tilde_2 = hyperbolic_tangent(z_c_tilde_2);

c_2 = f_2 * c_1 + i_2 * c_tilde_2;

tensor z_o_2 = matmul(w_o, concat2) + b_o;
tensor o_2 = sigmoid(z_o_2);

h_2 = o_2 * hyperbolic_tangent(c_2);
tensor y_2 = matmul(w_y, h_2) + b_y;

dL/dy_2 * dy_2/dh_2 * dh_2/do_2 * do_2/dconcat2 * dconcat2/dh_1 +
dL/dy_2 * dy_2/dh_2 * dh_2/dc_2 * dc_2/df_2 * df_2/dconcat2 * dconcat2/dh_1 +
dL/dy_2 * dy_2/dh_2 * dh_2/dc_2 * dc_2/di_2 * di_2/dconcat2 * dconcat2/dh_1 +
dL/dy_2 * dy_2/dh_2 * dh_2/dc_2 * dc_2/dc_tilde_2 * dc_tilde_2/dconcat2 * dconcat2/dh_1
= dL/dh_1

-------------------- Random --------------------

- I set batch_size = 8317 in lyrs.h since data size is (8327, 1) so batch size is 8327 - seq_length which is max batch size I believe. Its range is 1 ~ data size - sequence length.
- Maybe take batch_size as param in consturctor()? TF takes in train().

1 2 3 4 5 6 7 8 9 10
1 2 3, 2 3 4, 3 4 5, 4 5 6, 5 6 7, 6 7 8, 7 8 9
4, 5, 6, 7, 8, 9 10

1 2 3 4 5 6 7 8 9 10 11 12 13 14
1 2 3 4 5 6 7 8 9 10    -> 11
2 3 4 5 6 7 8 9 10 11   -> 12
3 4 5 6 7 8 9 10 11 12  -> 13
4 5 6 7 8 9 10 11 12 13 -> 14

I think it's 8317 instead...

(now)
50 1, 1 8316 = 50 8316 -> 50 50, 50 8316 = 50 8316 -> 1 50, 50 8316 = 1 8316
matmul(50 1, 1 8316) -> 50 8316 + matmul(50 50, 50 8316) -> 50 8316
matmul(1 50, 50 8316) -> 1 8316

8316 1, 1 50 = 8316 50 -> 8316 50, 50 50 = 8316 50 -> 8316 50, 50 1 = 8316 1

50 8316, 8316 1 = 50 1 -> 50 50, 50 1 = 50 1 -> 1 50, 50 1 = 1 1
I think this is wrong because when you think about it it's weird that getting only one ouput even thougth I input 8316 batches.