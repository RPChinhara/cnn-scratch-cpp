[ Why use matmul when taking partial derivatives respect to some parameters? ]

When batch size is 8317:
output_size = 1
batch_size  = 8317
hidden_size = 50

dl_d_y_pred = (output_size, batch_size)  d_y_pred_d_w_hy = (batch_size, hidden_size)
[ 1 2 3 ... 8317 ]                       [ 1 2 3 ... 50 ]
    imagine these as gradients(slope) -> / 2 .
                                      -> \ 3   .
                                      -> \ .     .
                                      -> / .       .
                                      -> \ .         .
                                         [ 8317         ]

------------------------------------------------------------------

When batch size is 1:
output_size = 1
batch_size  = 1
hidden_size = 50

dl_d_y_pred = (output_size, batch_size)  d_y_pred_d_w_hy = (batch_size, hidden_size)
[ 1 ]                                    [ 1 2 3 ... 50 ]

Imagine I have to calculate dl_d_why. In order to do that I have to get dl_d_y_pred and d_y_pred_d_w_hy, and then I have to take matmul on them, but why?
It is easier to imagine why matmul is used when batch size is 1. Since dl_d_y_pred is just a scaler in this case (1, 1). I'd just multiply this scaler into (1, 50) d_y_pred_d_w_hy.
Then I will get dl_d_why. It's pretty simple. However, things get bit different when the batch size is greater than 1, in this case 8317. The shape of result is same (1, 50).
I want to think of this calculation as adding gradients for each example as w_hy is used for all the examples. I add each examples(batch) so that it is taking account how each examples
affect w_hy which also affect loss. Also just remember that whenever I encounter batch_size in shape, most likely I should use matmul based on this assumption.

[ BPTT ]

BPTT process for an RNN with two time steps. This shows how to calculate gradients respect to Whh.

h1 = fh(matmul(Wxh, x1) + matmul(Whh, h0) + bh)
h2 = fh(matmul(Wxh, x2) + matmul(Whh, h1) + bh)
y  = fy(matmul(Why, h2) + by)

loss function at time step T = 2: L2 = L(y, y_true)

T = 2
dL2/dh2 = dL2/dy ⋅ dy/dh2 where dy/dh2 = Why
Thus, dL2/dh2 = dL2/dy ⋅ Why
dL2/dWhh (t = 2) = dL2/dh2 ⋅ dh2/dWhh where dh2/dWhh = squared(1 - h2) ⋅ transpose(h1)
Thus, dL2/dWhh = dL2/dh2 ⋅ squared(1 - h2) ⋅ transpose(h1)

T = 1
dL2/dh1 = dL2/dh2 ⋅ dh2/dh1 where dh2/dh1 = squared(1 - h2) ⋅ Whh
Thus, dL2/dh1 = dL2/dh2 ⋅ squared(1 - h2) ⋅ Whh
dL2/dWhh (t = 1) = dL2/dh1 ⋅ h1/dWhh where dh1/dWhh = squared(1 - h1) ⋅ transpose(h0)
Thus, dL2/dWhh = dL2/dh1 ⋅ squared(1 - h1) ⋅ transpose(h0)

Total gradient with respect to Whh

dL2/dWhh (t = 2) + dL2/dWhh (t = 1)
dL2/dWhh = (dL2/dh1 ⋅ squared(1 - h1) ⋅ transpose(h0)) + (dL2/dh2 ⋅ squared(1 - h2) ⋅ transpose(h1))


This is general equation of gradient with respect to Whh::
dL2/dWhh = sum t from 1 to 2 dL2/dht ⋅ dht/dWhh where dht/dWhh = squared(1 - ht) ⋅ transpose(ht-1)
Finally, dL2/dWhh = sum t from 1 to 2 dL2/dht ⋅ squared(1 - ht) ⋅ transpose(ht-1)

[ Random ]

- I set batch_size = 8317 in lyrs.h since data size is (8327, 1) so batch size is 8327 - seq_length which is max batch size I believe. Its range is 1 ~ data size - sequence length.
- Maybe take batch_size as param in consturctor()? TF takes in train().