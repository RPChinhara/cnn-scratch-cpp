[ Why use matmul when taking partial derivatives respect to some parameters? ]

When batch size is 8317:
output_size = 1
batch_size  = 8317
hidden_size = 50

dl_d_y_pred = (output_size, batch_size)  d_y_pred_d_w_hy = (batch_size, hidden_size)
[ 1 2 3 ... 8317 ]                       [ 1 2 3 ... 50 ]
    imagine these as gradients(slope) -> / 2 .
                                      -> \ 3   .
                                      -> \ .     .
                                      -> / .       .
                                      -> \ .         .
                                         [ 8317         ]

------------------------------------------------------------------

When batch size is 1:
output_size = 1
batch_size  = 1
hidden_size = 50

dl_d_y_pred = (output_size, batch_size)  d_y_pred_d_w_hy = (batch_size, hidden_size)
[ 1 ]                                    [ 1 2 3 ... 50 ]

Imagine I have to calculate dl_d_why. In order to do that I have to get dl_d_y_pred and d_y_pred_d_w_hy, and then I have to take matmul on them, but why?
It is easier to imagine why matmul is used when batch size is 1. Since dl_d_y_pred is just a scaler in this case (1, 1). I'd just multiply this scaler into (1, 50) d_y_pred_d_w_hy.
Then I will get dl_d_why. It's pretty simple. However, things get bit different when the batch size is greater than 1, in this case 8317. The shape of result is same (1, 50).
I want to think of this calculation as adding gradients for each example as w_hy is used for all the examples. I add each examples(batch) so that it is taking account how each examples
affect w_hy which also affect loss. Also just remember that whenever I encounter batch_size in shape, most likely I should use matmul based on this assumption.

[ BPTT ]

| This is when the model is many-to-one |

BPTT process for an RNN with two time steps. This shows how to calculate gradients respect to Whh.

h1 = fh(matmul(Wxh, x1) + matmul(Whh, h0) + bh)
h2 = fh(matmul(Wxh, x2) + matmul(Whh, h1) + bh)
y  = fy(matmul(Why, h2) + by)

loss function at time step T = 2: L2 = L(y, y_true)

T = 2
dL2/dh2 = dL2/dy ⋅ dy/dh2 where dy/dh2 = Why
Thus, dL2/dh2 = dL2/dy ⋅ Why
dL2/dWhh (t = 2) = dL2/dh2 ⋅ dh2/dWhh where dh2/dWhh = 1 - squared(h2) ⋅ transpose(h1)
Thus, dL2/dWhh = dL2/dh2 ⋅ 1 - squared(h2) ⋅ transpose(h1)

T = 1
dL2/dh1 = dL2/dh2 ⋅ dh2/dh1 where dh2/dh1 = 1 - squared(h2) ⋅ Whh
Thus, dL2/dh1 = dL2/dh2 ⋅ 1 - squared(h2) ⋅ Whh
dL2/dWhh (t = 1) = dL2/dh1 ⋅ h1/dWhh where dh1/dWhh = 1 - squared(h1) ⋅ transpose(h0)
Thus, dL2/dWhh = dL2/dh1 ⋅ 1 - squared(h1) ⋅ transpose(h0)

Total gradient with respect to Whh

dL2/dWhh (t = 2) + dL2/dWhh (t = 1)
dL2/dWhh = (dL2/dh1 ⋅ 1 - squared(h1) ⋅ transpose(h0)) + (dL2/dh2 ⋅ 1 - squared(h2) ⋅ transpose(h1))


This is general equation of gradient with respect to Whh::
dL2/dWhh = sum t from 1 to 2 dL2/dht ⋅ dht/dWhh where dht/dWhh = 1 - squared(ht) ⋅ transpose(ht-1)
Finally, dL2/dWhh = sum t from 1 to 2 dL2/dht ⋅ 1 - squared(ht) ⋅ transpose(ht-1)

| When the model is either one-to-many or many-to-many things are bit different |

For simplicity, let's say I'm calculating dL/dh1. The equation is dL/dh1 + dL/dh2 ⋅ dh2/dh1 since h1 is contributing both
loss at time step = 1, and future time step in this case which is h2.

[ Random ]

- I set batch_size = 8317 in lyrs.h since data size is (8327, 1) so batch size is 8327 - seq_length which is max batch size I believe. Its range is 1 ~ data size - sequence length.
- Maybe take batch_size as param in consturctor()? TF takes in train().

1 2 3 4 5 6 7 8 9 10
1 2 3, 2 3 4, 3 4 5, 4 5 6, 5 6 7, 6 7 8, 7 8 9
4, 5, 6, 7, 8, 9 10

1 2 3 4 5 6 7 8 9 10 11 12 13 14
1 2 3 4 5 6 7 8 9 10    -> 11
2 3 4 5 6 7 8 9 10 11   -> 12
3 4 5 6 7 8 9 10 11 12  -> 13
4 5 6 7 8 9 10 11 12 13 -> 14

I think it's 8317 instead...

(now)
50 1, 1 8316 = 50 8316 -> 50 50, 50 8316 = 50 8316 -> 1 50, 50 8316 = 1 8316
matmul(50 1, 1 8316) -> 50 8316 + matmul(50 50, 50 8316) -> 50 8316
matmul(1 50, 50 8316) -> 1 8316

8316 1, 1 50 = 8316 50 -> 8316 50, 50 50 = 8316 50 -> 8316 50, 50 1 = 8316 1

50 8316, 8316 1 = 50 1 -> 50 50, 50 1 = 50 1 -> 1 50, 50 1 = 1 1
I think this is wrong because when you think about it it's weird that getting only one ouput even thougth I input 8316 batches.