-------------------- BPTT --------------------

[This is when the model is many-to-one]

BPTT process for an RNN with two time steps. This shows how to calculate gradients respect to Whh.

h1 = fh(matmul(Wxh, x1) + matmul(Whh, h0) + bh)
h2 = fh(matmul(Wxh, x2) + matmul(Whh, h1) + bh)
y  = fy(matmul(Why, h2) + by)

loss function at time step T = 2: L = L(y, y_true)

T = 2
dL/dh2 = dL/dy ⋅ dy/dh2 where dy/dh2 = Why
Thus, dL/dh2 = dL/dy ⋅ Why
dL/dWhh (t = 2) = dL/dh2 ⋅ dh2/dWhh where dh2/dWhh = 1 - squared(h2) ⋅ transpose(h1)
Thus, dL/dWhh = dL/dh2 ⋅ 1 - squared(h2) ⋅ transpose(h1)

T = 1
dL/dh1 = dL/dh2 ⋅ dh2/dh1 where dh2/dh1 = 1 - squared(h2) ⋅ Whh
Thus, dL/dh1 = dL/dh2 ⋅ 1 - squared(h2) ⋅ Whh
dL/dWhh (t = 1) = dL/dh1 ⋅ h1/dWhh where dh1/dWhh = 1 - squared(h1) ⋅ transpose(h0)
Thus, dL/dWhh = dL/dh1 ⋅ 1 - squared(h1) ⋅ transpose(h0)

Total gradient with respect to Whh

dL/dWhh (t = 2) + dL/dWhh (t = 1)
dL/dWhh = (dL/dh1 ⋅ 1 - squared(h1) ⋅ transpose(h0)) + (dL/dh2 ⋅ 1 - squared(h2) ⋅ transpose(h1))


This is general equation of gradient with respect to Whh::
dL/dWhh = sum t from 1 to 2 dL/dht ⋅ dht/dWhh where dht/dWhh = 1 - squared(ht) ⋅ transpose(ht-1)
Thus, dL/dWhh = sum t from 1 to 2 dL/dht ⋅ 1 - squared(ht) ⋅ transpose(ht-1)

[When the model is either one-to-many or many-to-many things are bit different]

For simplicity, let's say I'm calculating dL/dh1. The equation is dL/dh1 + dL/dh2 ⋅ dh2/dh1 since h1 is contributing both
loss at time step = 1, and future time step in this case which is h2.

-------------------- Random --------------------

- I set batch_size = 8317 in lyrs.h since data size is (8327, 1) so batch size is 8327 - seq_length which is max batch size I believe. Its range is 1 ~ data size - sequence length.
- Maybe take batch_size as param in consturctor()? TF takes in train().

1 2 3 4 5 6 7 8 9 10
1 2 3, 2 3 4, 3 4 5, 4 5 6, 5 6 7, 6 7 8, 7 8 9
4, 5, 6, 7, 8, 9 10

1 2 3 4 5 6 7 8 9 10 11 12 13 14
1 2 3 4 5 6 7 8 9 10    -> 11
2 3 4 5 6 7 8 9 10 11   -> 12
3 4 5 6 7 8 9 10 11 12  -> 13
4 5 6 7 8 9 10 11 12 13 -> 14

I think it's 8317 instead...

(now)
50 1, 1 8316 = 50 8316 -> 50 50, 50 8316 = 50 8316 -> 1 50, 50 8316 = 1 8316
matmul(50 1, 1 8316) -> 50 8316 + matmul(50 50, 50 8316) -> 50 8316
matmul(1 50, 50 8316) -> 1 8316

8316 1, 1 50 = 8316 50 -> 8316 50, 50 50 = 8316 50 -> 8316 50, 50 1 = 8316 1

50 8316, 8316 1 = 50 1 -> 50 50, 50 1 = 50 1 -> 1 50, 50 1 = 1 1
I think this is wrong because when you think about it it's weird that getting only one ouput even thougth I input 8316 batches.