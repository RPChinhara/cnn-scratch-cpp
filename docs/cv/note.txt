Analysis:
----------------
- LeNet5 wasn't working, however after I used only 1 conv (1 ~ 2 output channels) and pooling, it started working. Is it because model is simply smaller?
- With 1 convolution (kernel size = (5, 1, 5, 5)) and max polling, the model is able to generalize.
- With 1 convolution (kernel size = (20, 1, 5, 5)) and max polling, the model is able to generalize. Though, loss is better when the number of output channels were 5. Loss started increasing after 5th epoch from 0.0447699 to 0.0619511, and to 0.0789601 at 6th epoch.
- With 1 convolution (kernel size = (50, 1, 5, 5)) and max polling, the model is able to generalize. However, it takes longer than previous two models.
- With 1 convolution (kernel size = (100, 1, 5, 5)) and max polling, the model is able to generalize. ???
- Losses around 0.06 ~ 0.07 indicated models have generalized to datasets?

Evaluation:
----------------
kernel1: (6, 1, 5, 5)
kernel2: (10, 6, 5, 5)
2 Convolutions, max pooling
relu activation
epochs: 100
lr: 0.0001f
batch size: 64

With these hyperparameters, it's a relatively good evaluation so far. Overfitted around 20 epochs.

Losses:
Epoch 1/100
938/938 - 141.751s/step - loss: 0.267637
Epoch 2/100
938/938 - 137.27s/step - loss: 0.220258
Epoch 3/100
938/938 - 139.516s/step - loss: 0.180697
Epoch 4/100
938/938 - 138.554s/step - loss: 0.143569
Epoch 5/100
938/938 - 137.351s/step - loss: 0.11399
Epoch 6/100
938/938 - 138.319s/step - loss: 0.0874346
Epoch 7/100
938/938 - 137.881s/step - loss: 0.0674631
Epoch 8/100
938/938 - 139.548s/step - loss: 0.0498639
Epoch 9/100
938/938 - 139.518s/step - loss: 0.0397332
Epoch 10/100
938/938 - 137.848s/step - loss: 0.0317425
Epoch 11/100
938/938 - 136.7s/step - loss: 0.028892
Epoch 12/100
938/938 - 138.137s/step - loss: 0.0260341
Epoch 13/100
938/938 - 140.418s/step - loss: 0.0234736
Epoch 14/100
938/938 - 140.955s/step - loss: 0.0220231
Epoch 15/100
938/938 - 139.881s/step - loss: 0.021524
Epoch 16/100
938/938 - 138.308s/step - loss: 0.0199542
Epoch 17/100
938/938 - 138.896s/step - loss: 0.0174806
Epoch 18/100
938/938 - 136.993s/step - loss: 0.0131259
Epoch 19/100
938/938 - 141.725s/step - loss: 0.0137896
Epoch 20/100
938/938 - 142.568s/step - loss: 0.0171674
Epoch 21/100
938/938 - 139.485s/step - loss: 0.0198975
Epoch 22/100
938/938 - 140.273s/step - loss: 0.0288678
Epoch 23/100
938/938 - 138.601s/step - loss: 0.0429514
Epoch 24/100
update todo!
Epoch 25/100
938/938 - 142.436s/step - loss: 0.057532
Epoch 26/100
938/938 - 140.454s/step - loss: 0.0421172
938/938 - 140.454s/step - loss: 0.0421172
Epoch 27/100
938/938 - 141.004s/step - loss: 0.030828
Epoch 28/100
938/938 - 140.518s/step - loss: 0.0240204
Epoch 29/100
328/938 - loss: 0.0967851    0 0 1 0 0 0 0 0 0 0     1.13718e-11 0.000105899 0.994727 0.000125623 1.83892e-09 2.44426e-09 5.85276e-10 0.00503941 2.03006e-06 2.48983e-08