Issues:
----------------
- How did they come up with attention or multiHeadAttention such as multiplying q and k, and later with v.
- What are weaknesses of the transformer?
    - Fix it, and make better one.
- What is Efficient Transformers?
- Do we really need encoders and decoders? Can I get rid of it?
- Can we make models without using datasets?
- How to solve needs billions of parameters (weights, biasese)?
    - Use bits as weights and biases?
    - Use one integet to represent 10 weights since maximum value for a variable of type int is 2147483647 which has 10 digits each act as weights.
    - Use smaller floating values
    - Just increase number of lyrs used? so each lyrs holds 1000 params times 1000 lyrs becomes 1000000 total params.
    - On average, human brain contains roughly 1000 billion neurons, but what about smart people like albert einstein. Are these ppl have more neurons?
- Make a pre-trained level models for image classification like ResNet and NLP like Llama
- Make a continuous & adaptive model rather than needs retraining on large datasets similar to current models?

Plans:
----------------
- Find weaknesses of existing models, and fix it to make new modesl.
- In the future, I need to save parameters like weights and biases, and keep updating these to make a better model like how transfer learning works.
    - I need to do this as soon as one can. Train a best model using same parameters, and keep updating these. I think once I created transformer I need to do this.
    - What happens if the model is overfitted? Add more training data so that losses will probably increase since model hasn't seen the datasets.
    - Search on what kind of datasets is chatGPT is trained on? Use these datasets to train mine.
    - However, sizes of weights and biases won't be same for all the models. What should I do?
    - I need to train for cv, nlp, rl?
        - I have to train using ImageNet for computer vision?
    - When I train it, do it on copy of dora, this way I can work on my project simultaneously.
    - Is transfer learning used repeadely to improve the model or just use pretrained model to train using unseen dataset to make a new model?
- If I made a best NLP model (maybe GPT), and train it for a year, will it surpass or be as close as chatGPT?
- I need to train embedding in order to create my own Word2Vec?
- I could implement traditional seq2seq for learning purpose comparing it with transformer so that I could comprehend why attention is emerged.
- I have to benchmark my model for different tasks, e.g., English (MMLU (EM), DROP (3-shot F1)), Code (HumanEval-Mul (Pass@1)), and Math (AIME 2024 (Pass@1)).
- Learn about the DeepSeek model.
- Make a system discards seen datasets so that it won't overfit with my method.

Comparisons:
----------------
- Obviously, RNNs are replaced by the Transformer, but are CNNs as well?
- Mamba is superior to Transformer.
- So once again how and why Transformer replaced RNNs? Should I really use Transformer? Why can't I go with RNNs?
    - The first for loop in forward() in rnn.cpp, which goes through number of seq_length which is 10 in this case, will only loop one time in Transformer I guess since it could be parallelized, and that is significant improvement.
    - So the improvement on the transformer is just speed that it became faster than RNNs? If I make a model faster than transformer, then it's superior to it? Is speed that essential for my physics goal?
    - In RNNs, unable to capture infos/semantics if the sequence of long was big, but transformer solved this.
- number of neurons
    - fly     = 139,255
    - gorilla = 33.4 billion
    - human   = 100 billion
    - GPT-3   = 175 billion
    - GPT-4   = 1.76 trillion

Performance Optimization:
----------------
- An alternative to backprop will increase perf.
- Why do we need all these parameters. Alternatives to it? Alternative ways to train models?
- I might need Fugaku or cloud computing for this.
- Attention and multiHeadAttention are just simple matmul ops, but in MultiHeadAttention, it processes inputs utilizing multile heads unlike RNNs which process sequentially which is pretty slow even though operations used are quite identical to simple matmul ops. For instance, simple rnn uses way less and simpler ops than multiHeadAttention.
    - Matmul is the key? -> It's kind of amazing how it's used so a lot, also transpose(). I thought, other ops in linalg are used.
        - Find or develop faster opration than it? This is essentially in math field.
    - I've implemented NNs, RNNs, CNNs, and now transformer, and I think they are pretty much same, yet from NNs, models are just getting more complex. Like programming language C, can we just use NNs to acheive things that other advanced models can? This simple NNs could be way faster...
- What can I do to increase perf? Make custom library for array and vector? Search it. I mean in general in CS, what can I do other than optimizing models.
- Is backprop parallelizable? What's standard?
    - If we can't parallelize backprop since each gradients depends on prevous ones because of the chain rule from math, we shouldn't be using it. Make alternative one.
- Dependency is really bad, weather it's from RNNs, CNNs or backprop. Operate everything in parallel if possible for performance.

Physics:
----------------
- Use a multimodal model to predict physical phenomena and solve unsolved problems, e.g., predict quantum gravity.
    - Given text, image, and video datasets as a multimodal inputs, predict next tokens. For instance, predict various unknown phenomena in unsolved physics problems.
    - Things to predict
        - Wave function
        - Near the black hole
        - Space time
- Will quantum neural network really work?
    - It needs quantum computers, and as of now it's an experimental stuff.
- Study about physics, and write on this file.
- All the language models like chatGPT, unless prompts are give, it won't initiate by itself, and this won't make a new theory like why gravity warps spacetime.