AGI
----------------
- Use a multimodal model to predict physical phenomena and solve unsolved problems, e.g., predict quantum gravity.
    - Given text, image, and video datasets as a multimodal inputs, predict next tokens. For instance, predict various unknown phenomena in unsolved physics problems.
    - Things to predict
        - Wave function
        - Near the black hole
        - Space time
- I might need Fugaku or cloud computing for this.
- Will quantum neural network really work?
    - It needs quantum computers, and as of now it's an experimental stuff.
- If I made a best NLP model (maybe GPT), and train it for a year, will it surpass or be as close as chatGPT?
- In the future, I need to save parameters like weights and biases, and keep updating these to make a better model like how transfer learning works.
    - I need to do this as soon as one can. Train a best model using same parameters, and keep updating these. I think once I created transformer I need to do this.
    - What happens if the model is overfitted? Add more training data so that losses will probably increase since model hasn't seen the datasets.
    - Search on what kind of datasets is chatGPT is trained on? Use these datasets to train mine.
    - However, sizes of weights and biases won't be same for all the models. What should I do?
    - I need to train for cv, nlp, rl?
        - I have to train using ImageNet for computer vision?
- Obviously, RNNs are replaced by the Transformer, but are CNNs as well?
- Mamba is superior to Transformer.
- So once again how and why Transformer replaced RNNs? Should I really use Transformer? Why can't I go with RNNs?
    - The first for loop in forward() in rnn.cpp, which goes through number of seq_length which is 10 in this case, will only loop one time in Transformer I guess since it could be parallelized, and that is significant improvement.
    - So the improvement on the transformer is just speed that it became faster than RNNs? If I make a model faster than transformer, then it's superior to it? Is speed that essential for my physics goal?
    - In RNNs, unable to capture infos/semantics if the sequence of long was big, but transformer solved this.
- Study about physics, and write on this file.
- All the language models like chatGPT, unless prompts are give, it won't initiate by itself, and this won't make a new theory like why gravity warps spacetime.