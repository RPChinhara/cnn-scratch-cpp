-------------------- Why use matmul when taking partial derivatives respect to some parameters? --------------------

When batch size is 8317:
output_size = 1
batch_size  = 8317
hidden_size = 50

dl_d_y_pred = (output_size, batch_size)  d_y_pred_d_w_hy = (batch_size, hidden_size)
[ 1 2 3 ... 8317 ]                       [ 1 2 3 ... 50 ]
    imagine these as gradients(slope) -> / 2 .
                                      -> \ 3   .
                                      -> \ .     .
                                      -> / .       .
                                      -> \ .         .
                                         [ 8317         ]

When batch size is 1:
output_size = 1
batch_size  = 1
hidden_size = 50

dl_d_y_pred = (output_size, batch_size)  d_y_pred_d_w_hy = (batch_size, hidden_size)
[ 1 ]                                    [ 1 2 3 ... 50 ]

Imagine I have to calculate dl_d_why. In order to do that I have to get dl_d_y_pred and d_y_pred_d_w_hy, and then I have to take matmul on them, but why?
It is easier to imagine why matmul is used when batch size is 1. Since dl_d_y_pred is just a scaler in this case (1, 1). I'd just multiply this scaler into (1, 50) d_y_pred_d_w_hy.
Then I will get dl_d_why. It's pretty simple. However, things get bit different when the batch size is greater than 1, in this case 8317. The shape of result is same (1, 50).
I want to think of this calculation as adding gradients for each example as w_hy is used for all the examples. I add each examples(batch) so that it is taking account how each examples
affect w_hy which also affect loss. Also just remember that whenever I encounter batch_size in shape, most likely I should use matmul based on this assumption.

-------------------- Dimensions of partial derivative --------------------

When I do calculation for partial derivative it is always confuses me since it have to much dimensions with parameters like w_xh, w_hh, and so on.
However, the key might use element-wise multiplication along with matmul, but maybe never sum(). Reason is that when you use sum() it will average out all the gradients from the batch which
will leads to losing precision, and then you add this averaged gradients to parameters like w_xh which is not precise. So, it's almost always recommended to much dimentions using element-wise multiplication.

Another insight for muching Dimensions
Imagine following 3 matrices for below shapes,

batch_size  = 1
hidden_size = 50

(1, 50), (50, 1), (50, 50)

I have to multiply these to get final shape of (1, 50), so it will be either

element-wise multiplication on (1, 50) and transpose(50, 1), and take matmul on previous result and (50, 50) or,
element-wise multiplication on transpose(1, 50) and transpose(50, 1), and take matmul on transpose of previous result and (50, 50).

It's obviously clear that matmul on (1, 50) and (50, 1) is wrong since it'd result in (1, 1), then I won't be able to take matmul with (50, 50) although I could just multiply (1, 1) and (50, 50), but it won't lead to (1, 50).

What I want to say here is that I think you don't want to mess with hidden_size ever. You don't want to counduct operations that destroy shape for hidden_size just keep shape for hidden_size, however I could mess with shapes for batch_size.